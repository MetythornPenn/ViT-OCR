{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64ecc8ae",
   "metadata": {},
   "source": [
    "## Vision Transformer apply for OCR task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b230719c",
   "metadata": {},
   "source": [
    "+ [ViT tutorial](https://www.datacamp.com/tutorial/vision-transformers)\n",
    "+ [multi-head attenion](https://www.datacamp.com/tutorial/multi-head-attention-transformers)\n",
    "+ [Transformer block in Pytorch](https://docs.pytorch.org/docs/2.9/generated/torch.nn.Transformer.html)\n",
    "+ [Building a Vision Transformer Model From Scratch](https://medium.com/correll-lab/building-a-vision-transformer-model-from-scratch-a3054f707cc6)\n",
    "+ [Understanding Multi-Head Attention in Transformers](https://www.datacamp.com/tutorial/multi-head-attention-transformers)\n",
    "+ [Transformer Course from Standford](https://www.youtube.com/watch?v=P127jhj-8-Y&list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM)\n",
    "+ [Paper : Attension Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "+ [Paper : Vision Transformer](https://arxiv.org/abs/2010.11929)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a015bb2",
   "metadata": {},
   "source": [
    "## Load libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6f599af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from torch.optim import Adam\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf672ea",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66fb1bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7647a05d",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6085db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 64, 256])\n"
     ]
    }
   ],
   "source": [
    "# Patch EMbeddings\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, img_size, patch_size, n_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model # Dimension of the model\n",
    "        self.img_size = img_size # Size (H, W)\n",
    "        self.patch_size = patch_size # (Ph, Pw)\n",
    "        self.n_channels = n_channels #  3 for RGB, 1 for Grayscale\n",
    "        \n",
    "        self.linear_project = nn.Conv2d(self.n_channels, self.d_model, kernel_size=self.patch_size, stride=self.patch_size)\n",
    "    # B: Batch Size\n",
    "    # C: Image Channels\n",
    "    # H: Image Height\n",
    "    # W: Image Width\n",
    "    # P_col: Patch Column\n",
    "    # P_row: Patch Row\n",
    "    def forward(self, x):\n",
    "        x = self.linear_project(x) # (B, C, H, W) -> (B, d_model, P_col, P_row)\n",
    "        x = x.flatten(2) # (B, d_model, P_col, P_row) -> (B, d_model, P)\n",
    "        x = x.transpose(1, 2) # (B, d_model, P) -> (B, P, d_model)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "# example usage:\n",
    "img = torch.randn(8, 3, 128, 128) # (B, C, H, W)\n",
    "patch_embed = PatchEmbedding(d_model=256, img_size=(128, 128), patch_size=(16, 16), n_channels=3)\n",
    "patches = patch_embed(img) # (B, P, d)\n",
    "print(patches.shape) # should be (8, 64, 256) since there\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d865918b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 64, 256])\n"
     ]
    }
   ],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_height, img_width, patch=16, d_model=256):\n",
    "        super().__init__()\n",
    "        assert img_height % patch == 0 and img_width % patch == 0, \\\n",
    "            f\"Image H/W must be divisible by patch size {patch}\"\n",
    "        self.patch = patch\n",
    "        self.num_patches = (img_height // patch) * (img_width // patch)\n",
    "        self.d_model = d_model\n",
    "        self.proj = nn.Linear(3 * patch * patch, d_model)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches, d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,3,H,W)\n",
    "        B, C, H, W = x.shape\n",
    "        p = self.patch\n",
    "        \n",
    "        # Cut image into patches\n",
    "        patches = x.unfold(2,p,p).unfold(3,p,p)  # B,C,Hp,Wp,p,p\n",
    "        patches = patches.permute(0,2,3,1,4,5).contiguous()  # B,Hp,Wp,C,p,p\n",
    "        patches = patches.view(B, -1, C*p*p)  # B, Np, C*p*p\n",
    "        patches = self.proj(patches)  # B, Np, d_model\n",
    "        patches = patches + self.pos_embed  # add positional encoding\n",
    "\n",
    "        return patches  # (B, Np, d)\n",
    "    \n",
    "    \n",
    "# example usage:\n",
    "img = torch.randn(8, 3, 128, 128) # (B, C, H, W)\n",
    "patch_embed = PatchEmbedding(img_height=128, img_width=128, patch=16, d_model=256)\n",
    "patches = patch_embed(img) # (B, P, d)\n",
    "print(patches.shape) # should be (8, 64, 256) since there"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da50fb6",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4c49d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74037f75",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a9b1ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-stack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
