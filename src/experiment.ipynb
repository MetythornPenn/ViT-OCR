{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38a33128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "DEFAULT_CONFIG = {\n",
    "    # Supports: \"path/img.jpg Label\" OR \"path/img.jpg\\tLabel\"\n",
    "    \"data_file\": \"/home/metythorn/konai/services/ocr-service/data/processed/text-recognition/ocr_dataset_v2.txt\",\n",
    "    \"img_height\": 128,\n",
    "    \"patch_size\": 16,\n",
    "    \"dim_feedforward\": 2048,\n",
    "    \"dropout\": 0.1,\n",
    "    \"augment\": True,\n",
    "    \"max_decode_len\": 150,\n",
    "    \"quiet\": False,\n",
    "    \"device\": \"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
    "}\n",
    "\n",
    "# Adjust epoch base on ur GPU VRAM (I'm using RTX 5090 32GB)\n",
    "MODEL_VARIANTS = {\n",
    "    \"small\": {\n",
    "        \"img_width\": 320,\n",
    "        \"batch_size\": 128,\n",
    "        \"lr\": 1e-4,\n",
    "        \"epochs\": 10,\n",
    "        \"d_model\": 384,\n",
    "        \"nhead\": 6,\n",
    "        \"num_layers\": 6,\n",
    "        \"patch_size\": 16,\n",
    "        \"dim_feedforward\": 1536,\n",
    "        \"checkpoint_dir\": \"checkpoints_small\",\n",
    "    },\n",
    "    \"base\": {\n",
    "        \"img_width\": 384,\n",
    "        \"batch_size\": 90,\n",
    "        \"lr\": 8e-5,\n",
    "        \"epochs\": 8,\n",
    "        \"d_model\": 768,\n",
    "        \"nhead\": 12,\n",
    "        \"num_layers\": 10,\n",
    "        \"patch_size\": 16,\n",
    "        \"dim_feedforward\": 3072,\n",
    "        \"checkpoint_dir\": \"checkpoints_base\",\n",
    "    },\n",
    "    \"large\": {\n",
    "        \"img_width\": 384,\n",
    "        \"batch_size\": 64,\n",
    "        \"lr\": 6e-5,\n",
    "        \"epochs\": 8,\n",
    "        \"d_model\": 1024,\n",
    "        \"nhead\": 16,\n",
    "        \"num_layers\": 12,\n",
    "        \"patch_size\": 16,\n",
    "        \"dim_feedforward\": 4096,\n",
    "        \"checkpoint_dir\": \"checkpoints_large\",\n",
    "    },\n",
    "}\n",
    "\n",
    "CONFIG = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f56d2ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_config(vocab, path=None):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4340d37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-stack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
